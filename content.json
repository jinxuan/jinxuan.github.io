{"meta":{"title":"Jinxuan Wu's Blog","subtitle":null,"description":null,"author":"Jinxuan Wu","url":"http://jinxuan.github.io"},"pages":[{"title":"categories","date":"2017-09-14T05:23:36.000Z","updated":"2017-09-14T05:24:52.000Z","comments":true,"path":"categories/index.html","permalink":"http://jinxuan.github.io/categories/index.html","excerpt":"","text":"","raw":null,"content":null},{"title":"tags","date":"2017-09-14T06:28:11.000Z","updated":"2017-09-14T06:28:43.000Z","comments":true,"path":"tags/index.html","permalink":"http://jinxuan.github.io/tags/index.html","excerpt":"","text":"","raw":null,"content":null}],"posts":[{"title":"Database Meets Deep Learning: Challenges and Opportunities","slug":"Database-Meets-Deep-Learning-Challenges-and-Opportunities","date":"2017-09-19T06:52:02.000Z","updated":"2017-09-19T07:28:31.000Z","comments":true,"path":"2017/09/18/Database-Meets-Deep-Learning-Challenges-and-Opportunities/","link":"","permalink":"http://jinxuan.github.io/2017/09/18/Database-Meets-Deep-Learning-Challenges-and-Opportunities/","excerpt":"AbstractDeep learning has recently become very popular on account of its incredible success in many complex data- driven applications, including image classiﬁcation and speech recognition. The database community has worked on data-driven applications for many years, and therefore should be playing a lead role in supporting this new wave. However, databases and deep learning are differ- ent in terms of both techniques and applications. In this paper, we discuss research problems at the intersection of the two ﬁelds. In particular, we discuss possible improvements for deep learning systems from a database perspective, and analyze database applications that may beneﬁt from deep learning techniques.","text":"AbstractDeep learning has recently become very popular on account of its incredible success in many complex data- driven applications, including image classiﬁcation and speech recognition. The database community has worked on data-driven applications for many years, and therefore should be playing a lead role in supporting this new wave. However, databases and deep learning are differ- ent in terms of both techniques and applications. In this paper, we discuss research problems at the intersection of the two ﬁelds. In particular, we discuss possible improvements for deep learning systems from a database perspective, and analyze database applications that may beneﬁt from deep learning techniques. Introduction These days, we witnessed the success of numerous data-driven machine-learning-based applications. Insights that the database community can offer to deep learning? Apply database techniques for optimizing deep learn- ing systems. Donducted using deep learning techniques for database problems. Such as knowledge fusion and crowdsourcing, which are probabilistic problems. It is possible to apply deep learning techniques in these areas. DATABASES TO DEEP LEARNING Stand-alone Training Use Nvidia GPU with the cuDNN library Operation Scheduling, build a cost model for operation placing strategy like query plans estimate in database. Memory Management, such as use paging and cacheing techniques in DB to GPU Memory management. Distribute Training The parameter server architecture is typically used, in which the workers compute parameter gradients and the servers update the parameter val- ues after receiving gradients from workers. There are two basic parallelism schemes for distributed training, namely, data parallelism and model par- allelism. In data parallelism, each worker is as- signed a data partition and a model replica, while for model parallelism, each worker is assigned a par- tition of the model and the whole dataset. * Communication and Synchronization * Concurrency and Consistency * Fault Tolerance DEEP LEARNING TO DATABASES[todo] #ConclusionDatabases have many techniques for optimizing system performance, while deep learning is good at learning e↵ective representation for data-driven applications. We note that these two “different” areas share some common techniques for improving the system performance, such as memory optimization and parallelism. We have discussed some possible improvements for deep learning sys- tems using database techniques, and research problems applying deep learning techniques in database applications","raw":null,"content":null,"categories":[{"name":"paper reading","slug":"paper-reading","permalink":"http://jinxuan.github.io/categories/paper-reading/"}],"tags":[]},{"title":"Revisiting the Design of Data Stream Processing Systems on Multi-Core Processors","slug":"Revisiting-the-Design-of-Data-Stream-Processing-Systems-on-Multi-Core-Processors","date":"2017-09-17T22:49:54.000Z","updated":"2017-09-17T22:49:54.000Z","comments":true,"path":"2017/09/17/Revisiting-the-Design-of-Data-Stream-Processing-Systems-on-Multi-Core-Processors/","link":"","permalink":"http://jinxuan.github.io/2017/09/17/Revisiting-the-Design-of-Data-Stream-Processing-Systems-on-Multi-Core-Processors/","excerpt":"","text":"","raw":null,"content":null,"categories":[],"tags":[]},{"title":"Building Connected Car Applications on Top of the World-Wide Streams Platform","slug":"Building-Connected-Car-Applications-on-Top-of-the-World-Wide-Streams-Platform","date":"2017-09-16T06:30:22.000Z","updated":"2017-09-16T06:30:45.000Z","comments":true,"path":"2017/09/15/Building-Connected-Car-Applications-on-Top-of-the-World-Wide-Streams-Platform/","link":"","permalink":"http://jinxuan.github.io/2017/09/15/Building-Connected-Car-Applications-on-Top-of-the-World-Wide-Streams-Platform/","excerpt":"","text":"Demo: Building Connected Car Applications on Top of the World-Wide Streams PlatformAbstractThe connected car is likely to play a fundamental role in the fore- seeable Internet of ings. e connectivity aspect in combination with the available data (e.g. from GPS, on-board diagnostics, road sensors) and video (e.g. from dashcams and trac cameras) streams enable a range of new applications, e.g., accident avoidance, online route planning, energy optimization, etc. ese applications, however, come with an additional set of re- quirements which are not accommodated by the state-of-the-art stream processing platforms. We have built World-Wide Streams (WWS), a novel stream processing platform that has been explicitly designed with those requirements in mind. In this demo presenta- tion, we will show a number of connected car scenarios that we have built on top of WWS.","raw":null,"content":null,"categories":[{"name":"paper reading","slug":"paper-reading","permalink":"http://jinxuan.github.io/categories/paper-reading/"}],"tags":[]},{"title":"Tutorial: Reflections on Almost Two Decades of Research into Stream Processing","slug":"Tutorial-Reflections-on-Almost-Two-Decades-of-Research-into-Stream-Processing","date":"2017-09-15T04:28:26.000Z","updated":"2017-09-17T23:26:53.000Z","comments":true,"path":"2017/09/14/Tutorial-Reflections-on-Almost-Two-Decades-of-Research-into-Stream-Processing/","link":"","permalink":"http://jinxuan.github.io/2017/09/14/Tutorial-Reflections-on-Almost-Two-Decades-of-Research-into-Stream-Processing/","excerpt":"#Tutorial: Reflections on Almost Two Decades of Research into Stream Processing\nThis tutorial reflects on this research history by highlighting a number of trends and best practices that can be identied in hindsight. It also enumerates a list of directions for future research in stream processing.","text":"#Tutorial: Reflections on Almost Two Decades of Research into Stream Processing This tutorial reflects on this research history by highlighting a number of trends and best practices that can be identied in hindsight. It also enumerates a list of directions for future research in stream processing. NOTABLE SYSTEMS: TinyDB STREAM TelegraphCQ [13], Gigascope [15], Aurora Borealis IBM Infosphere Systems System S S4 Storm Spark Streaming Flink Kafka/Samza Millwheel Google Cloud DataFlow StreamScope TRENDS From DSMSs to Big “Streaming” Data Frameworks Combine the low-latency benefits of streaming systems with the scalability properties of Big Data frameworks. Increased Importance of Exact Results providing the exactly once delivery semantics. Even transactional properties One-pass Computation vs Replayability Previously data was primarily kept in memory but for a very short period of time for low latency requirement assumption. Now use high-throughput persisted message buses such as Kafka ease of debugging and replayability. Simplifies fault-tolerance mechanisms. Unification of Batch and Streaming Models Flink and apache Beam. Domain-specific to General-purpose wireless sensor networks or network traffic monitoring Richer Window Specifications Time or count-based windows Session Windows (Google Cloud Dataflow and Kafka Streams) and Episodes/Frames ##BEST PRACTICES System-wide State Management Simplified Reasoning via Punctuation","raw":null,"content":null,"categories":[{"name":"paper reading","slug":"paper-reading","permalink":"http://jinxuan.github.io/categories/paper-reading/"}],"tags":[]},{"title":"Continuous Queries over Data Streams","slug":"Continuous Queries over Data Streams","date":"2017-09-14T05:32:28.000Z","updated":"2017-09-14T06:12:16.000Z","comments":true,"path":"2017/09/13/Continuous Queries over Data Streams/","link":"","permalink":"http://jinxuan.github.io/2017/09/13/Continuous Queries over Data Streams/","excerpt":"Continuous Queries over Data StreamsAbstractIn many recent applications, data may take the form of continuous data streams,   rather than  nite stored data sets.   Several aspects of data management need to be re- considered in the presence of data streams, offering a new research direction for the database community. In this pa- per we focus primarily on the problem of query process- ing, specically on how to dene and evaluate continuous queries over data streams.   We address semantic issues as well as efciency concerns. Our main contributions are threefold. First, we specify a general and exible architec- ture for query processing in the presence of data streams. Second, we use our basic architecture as a tool to clar- ify alternative semantics and processing techniques for continuous queries.   The architecture also captures most previous work on continuous queries and data streams, as well as related concepts such as triggers and materialized views. Finally, we map out research topics in the area of query processing over data streams, showing where pre- vious work is relevant and describing problems yet to be addressed.","text":"Continuous Queries over Data StreamsAbstractIn many recent applications, data may take the form of continuous data streams, rather than nite stored data sets. Several aspects of data management need to be re- considered in the presence of data streams, offering a new research direction for the database community. In this pa- per we focus primarily on the problem of query process- ing, specically on how to dene and evaluate continuous queries over data streams. We address semantic issues as well as efciency concerns. Our main contributions are threefold. First, we specify a general and exible architec- ture for query processing in the presence of data streams. Second, we use our basic architecture as a tool to clar- ify alternative semantics and processing techniques for continuous queries. The architecture also captures most previous work on continuous queries and data streams, as well as related concepts such as triggers and materialized views. Finally, we map out research topics in the area of query processing over data streams, showing where pre- vious work is relevant and describing problems yet to be addressed. Introduction","raw":null,"content":null,"categories":[{"name":"paper reading","slug":"paper-reading","permalink":"http://jinxuan.github.io/categories/paper-reading/"}],"tags":[]},{"title":"Exploring big volume sensor data with Vroom","slug":"Exploring-big-volume-sensor-data-with-Vroom","date":"2017-09-14T05:32:28.000Z","updated":"2017-09-14T06:14:54.000Z","comments":true,"path":"2017/09/13/Exploring-big-volume-sensor-data-with-Vroom/","link":"","permalink":"http://jinxuan.github.io/2017/09/13/Exploring-big-volume-sensor-data-with-Vroom/","excerpt":"Exploring big volume sensor data with VroomComments:ABSTRACT State of the art sensors within a single autonomous vehicle (AV) can produce video and LIDAR data at rates greater than 30 GB/hour. Unsurprisingly, even small AV research teams can accumulate tens of terabytes of sensor data from multiple trips and multiple vehicles. AV practitioners would like to extract information about specic locations or specic situations for further study, but are often unable to. Queries over AV sensor data are dierent from generic analytics or spatial queries because they demand reasoning about elds of view as well as heavy computation to extract features from scenes. In this article and demo we present Vroom, a system for ad-hoc queries over AV sensor databases. Vroom combines domain specic properties of AV datasets with selective indexing and multi-query optimization to address challenges posed by AV sensor data.","text":"Exploring big volume sensor data with VroomComments:ABSTRACT State of the art sensors within a single autonomous vehicle (AV) can produce video and LIDAR data at rates greater than 30 GB/hour. Unsurprisingly, even small AV research teams can accumulate tens of terabytes of sensor data from multiple trips and multiple vehicles. AV practitioners would like to extract information about specic locations or specic situations for further study, but are often unable to. Queries over AV sensor data are dierent from generic analytics or spatial queries because they demand reasoning about elds of view as well as heavy computation to extract features from scenes. In this article and demo we present Vroom, a system for ad-hoc queries over AV sensor databases. Vroom combines domain specic properties of AV datasets with selective indexing and multi-query optimization to address challenges posed by AV sensor data. ##Introduction AV generate from high-resolution cameras, lidar and GPS at about 10 MBps. Queries: Q1 Compute basic statistics on recent trips such as datarates by sensor and location coverage. Q2 [building 3D maps] Retrieve all forward-facing video frames of the corner of Vassar and Main St. in Cambridge, MA., ordered clockwise. Q3 [ preparing labeled training] Retrieve lidar and video readings for all cameras in the vehicle, for intervals when any vehicle camera frame shows a bicycle. Group the data by trip, and order it by timestamp within each trip. Q4 [ preparing labeled training] Retrieve all sensor readings in the minute leading up to an interesting event, such as a possible near miss. e.g., where a vehicle’s CAN bus records a sudden brake or sharp steer, group the readings by trip and order them by timestamp within each trip. Challenges Computational intensity of UDFs: such as deep learning based classification Big volumes: ad-hoc query on large historical data Many features of interest: Interface and storage issues: Architecture Sophisticated feature precomputation and indexing: Synthesizing cheap predicates: Memoizing: Storage clustering, based on the workload Multi-query optimization: [to read] polystore data model SystemQuery InterfaceQ1:12345678select sensor_id, sensor_type sum(byte_size(sensor_reading)) as data_volume, data_volume/trip_duration as data_rate, count(*)/trip_duration as frequency from raw_data where time.now - trip_start &lt; 6 days group by trip_id, sensor_id order by trip_id, data_rate desc Q2:1234567let vassar_and_main = lat_lon_height(42.3628,-71.0915,7) INSELECT sensor_readingFROM raw_dataWHERE sensor_reading.type IN (VideoFrame) AND let sensor_pose = pose_estimate(sensor_id, TIMESTAMP) IN distance(sensor_pose, vassar_and_main) &lt; 20 AND angle(sensor_pose.x_axis, line(sensor_pose, vassar_and_main)) &lt; 30ORDER BY angle(line(sensor_pose, std.east), line(sensor_pose, vassar_and_main)) Q3:12345678910111213let bike_segments =SELECT trip_id, TIMESTAMP - 5 AS t_start, TIMESTAMP + 5 AS t_endFROM raw_dataWHERE sensor_reading.type IN (VideoFrame) AND bike_detection_udf(sensor_reading) &gt; 0.9 IN SELECT DISTINCT TIMESTAMP, trip_id, sensor_reading FROM raw_data, bike_segments WHERE sensor_reading.type (PointCloud, Video) AND raw_data.trip_id = bike_segments.trip_id AND raw_data.TIMESTAMP BETWEEN (t_start, t_end) ANDORDER BY trip_id, TIMESTAMP Storage enginesOur demonstration prototype is implemented by combining a relational engine for metadata storage and file system based blob management. Query processor For Query Q1, a per trip aggregate triggers checks for existing per-trip memoized computations. For Query Q2 explicitly uses geometric builtins referring to sensor points of view, geometric predicates involved and bound which parts of trip trajectories to skip completely look at. We leverage existing trajectory indexing work for storage.For Query q3:","raw":null,"content":null,"categories":[{"name":"paper reading","slug":"paper-reading","permalink":"http://jinxuan.github.io/categories/paper-reading/"}],"tags":[]},{"title":"Geospatial Stream Query Processing using Microsoft SQL Server StreamInsight","slug":"Geospatial Stream Query Processing using Microsoft SQL Server StreamInsight","date":"2017-09-14T05:32:28.000Z","updated":"2017-09-14T06:12:53.000Z","comments":true,"path":"2017/09/13/Geospatial Stream Query Processing using Microsoft SQL Server StreamInsight/","link":"","permalink":"http://jinxuan.github.io/2017/09/13/Geospatial Stream Query Processing using Microsoft SQL Server StreamInsight/","excerpt":"Geospatial Stream Query Processing using Microsoft SQL Server StreamInsightAbstract Microsoft SQL Server spatial libraries contain several components that handle geometrical and geographical data types. With advances in geo-sensing technologies, there has been an increasing demand for geospatial streaming applications. Microsoft SQL Server StreamInsight (StreamInsight, for brevity) is a platform for developing and deploying streaming applications that run continuous queries over high-rate streaming events. With its extensibility infrastructure, StreamInsight enables developers to integrate their domain expertise within the query pipeline in the form of user defined modules. This demo utilizes the extensibility infrastructure in Microsoft StreamInsight to leverage its continuous query processing capabilities in two directions. The first direction integrates SQL spatial libraries into the continuous query pipeline of StreamInsight. StreamInsight provides a well-defined temporal model over incoming events while SQL spatial libraries cover the spatial properties of events to deliver a solution for spatiotemporal stream query processing. The second direction extends the system with an analytical refinement and prediction layer. This layer analyzes historical data that has been accumulated and summarized over the years to refine, smooth and adjust the current query output as well as predict the output in the near future. The demo scenario is based on transportation data in Los Angeles County.","text":"Geospatial Stream Query Processing using Microsoft SQL Server StreamInsightAbstract Microsoft SQL Server spatial libraries contain several components that handle geometrical and geographical data types. With advances in geo-sensing technologies, there has been an increasing demand for geospatial streaming applications. Microsoft SQL Server StreamInsight (StreamInsight, for brevity) is a platform for developing and deploying streaming applications that run continuous queries over high-rate streaming events. With its extensibility infrastructure, StreamInsight enables developers to integrate their domain expertise within the query pipeline in the form of user defined modules. This demo utilizes the extensibility infrastructure in Microsoft StreamInsight to leverage its continuous query processing capabilities in two directions. The first direction integrates SQL spatial libraries into the continuous query pipeline of StreamInsight. StreamInsight provides a well-defined temporal model over incoming events while SQL spatial libraries cover the spatial properties of events to deliver a solution for spatiotemporal stream query processing. The second direction extends the system with an analytical refinement and prediction layer. This layer analyzes historical data that has been accumulated and summarized over the years to refine, smooth and adjust the current query output as well as predict the output in the near future. The demo scenario is based on transportation data in Los Angeles County. Introduction geostreaming, StreamLight, Spatial Libraries Challenges port geospatial libraries to the streaming domain with the incremental single-pass processing model in mind. make decision based on the continuous query result. Contributions integrate Microsoft SQL Server Spatial Libraries into Microsoft StreamInsight to support the online processing of geo-stream data. Special attention is given to incrementally evaluate spatial operations. Implement an online analytical refinement and prediction layer that enables querying of historical (archived) stream data. ArchitectureStreamInsight Input Adapter: continuously listens on a TCP port for the streaming data. Once a packet of data is retrieved, the corresponding events are created and enqueued to the streaming engine. Streaming Engine: Runs pre-defined queries on input events, Also did query fusing, operator sharing, and query and stream partitioning. StreamInsight provides user-defined aggregate (UDA), user- defined operator (UDO), and user-defined function (UDF) facilities. Output Adapter Online Analytical Refinement and Prediction Layer Use PCA to implement a historical data sketches. more correlated is the streaming data, the less number of components are needed to create accurate . Given efficient access to historical data through UDAs, one can accomplish the following refinement and prediction functions. Refinement functions Substituting missing streaming data Smoothing noisy input data Detection of anomalies Prediction functions Predicting near future trends Responding to anomalies Spatial Cartridge For stream join relational table Spatial cartridge extends the capabilities of StreamInsight Determining the way StreamInsight retrieves and interprets the underlying spatial information (e.g., road networks) by customizing the spatial operators and idexes for efficient access to large spatial data. This way users can easily implement the functions or interfaces that have the specialized behavior required in the geostreaming applications.","raw":null,"content":null,"categories":[{"name":"paper reading","slug":"paper-reading","permalink":"http://jinxuan.github.io/categories/paper-reading/"}],"tags":[]},{"title":"SOLE Scalable On-Line Execution of Continuous Queries on Spatio-temporal Data Streams","slug":"SOLE: Scalable On-Line Execution of Continuous Queries on Spatio-temporal Data Streams","date":"2017-09-14T05:32:28.000Z","updated":"2017-09-14T06:12:25.000Z","comments":true,"path":"2017/09/13/SOLE: Scalable On-Line Execution of Continuous Queries on Spatio-temporal Data Streams/","link":"","permalink":"http://jinxuan.github.io/2017/09/13/SOLE: Scalable On-Line Execution of Continuous Queries on Spatio-temporal Data Streams/","excerpt":"SOLE: Scalable On-Line Execution of Continuous Queries on Spatio-temporal Data StreamsAbstract This   paper   presents   the   Scalable   On-Line Execution   algorithm   (SOLE,   for   short)   for   continuous and on-line evaluation of concurrent continuous spatio- temporal   queries   over   data   streams.   Incoming   spatio- temporal data streams are processed in-memory against a set of outstanding continuous queries. The SOLE algo- rithm utilizes the scarce memory resource eciently by keeping track of only the signicant objects. In-memory stored objects are expired (i.e., dropped) from memory once they become insignicant. SOLE is a scalable algo- rithm where all the continuous outstanding queries share the   same   buer   pool.   In   addition,   SOLE   is   presented as   a   spatio-temporal   join   between   two   input   streams, a   stream   of   spatio-temporal   objects   and   a   stream   of spatio-temporal queries. To cope with intervals of high arrival rates of objects and/or queries, SOLE utilizes a load-shedding approach where some of the stored objects are dropped from memory. SOLE is implemented   as a pipelined query operator that can be combined with tra- ditional query operators in a query execution plan to sup- port a wide variety of continuous queries. Performance experiments based on a real implementation of SOLE in- side a prototype of a data stream management system show the scalability and eciency of SOLE in highly dynamic environments.","text":"SOLE: Scalable On-Line Execution of Continuous Queries on Spatio-temporal Data StreamsAbstract This paper presents the Scalable On-Line Execution algorithm (SOLE, for short) for continuous and on-line evaluation of concurrent continuous spatio- temporal queries over data streams. Incoming spatio- temporal data streams are processed in-memory against a set of outstanding continuous queries. The SOLE algo- rithm utilizes the scarce memory resource eciently by keeping track of only the signicant objects. In-memory stored objects are expired (i.e., dropped) from memory once they become insignicant. SOLE is a scalable algo- rithm where all the continuous outstanding queries share the same buer pool. In addition, SOLE is presented as a spatio-temporal join between two input streams, a stream of spatio-temporal objects and a stream of spatio-temporal queries. To cope with intervals of high arrival rates of objects and/or queries, SOLE utilizes a load-shedding approach where some of the stored objects are dropped from memory. SOLE is implemented as a pipelined query operator that can be combined with tra- ditional query operators in a query execution plan to sup- port a wide variety of continuous queries. Performance experiments based on a real implementation of SOLE in- side a prototype of a data stream management system show the scalability and eciency of SOLE in highly dynamic environments. Introduction The high arrival rates of spatio-temporal data streams along with its massive data sizes Exiting techniques for spatio- temporal databases rely mainly on the basic assumption that all incoming spatio-temporal data can be stored on disk. Thus, continuous query processing techniques aim to utilize the disk storage to produce in- cremental results of continuous queries. combine traditional spatio-temporal query processors and data stream query processors. process uncertainty areas using aconservative caching technique. load shedding schemes to support larger numbers of continuous queries Related work Spatio-temporal Databases Existing algorithm materializing incoming spatio-temporal data in disk-based index structures Memory-based data structures have been proposed in to deal with reasonable size of data that can fit in memory, but it is not scalable to large data sizes or streaming environments. (to add) Data Stream Management Systems the spatial and temporal properties of data streams and/or continuous queries are overlooked by existing stream management prototypes Scalable execution of continuous queries in tradi- tional data streams aims to either detect common subex- pressions or share resources at the operator level. The main idea is to either add a special operator to the query plan to regu- late the load by discarding unimportant incoming tuples or dynamically adjust the window size and time granu- larity at runtime. Basic Concepts in SOLEInput Output Input Model. The inputs to SOLE are two streams: 1. A spaial temporal stream 2. A stream of continuous queries Output Model. avoid continuous reevaluation of continuous spatio- temporal queries, updates the query re- sult by computing and sending only updates of the pre- viously reported answer. Positive updates, negative update Supporting Various Query Types Moving Queries. kNN Queries, (Use variable circle region size) Pipelined Operator physical pipelined operator that can interact with traditional query operators in a large pipelined query plan. 4 Execution of Single Continuous Queries in SOLE","raw":null,"content":null,"categories":[{"name":"paper reading","slug":"paper-reading","permalink":"http://jinxuan.github.io/categories/paper-reading/"}],"tags":[]},{"title":"","slug":"Exploring big volume sensor data with Vroom","date":"2017-09-13T07:00:00.000Z","updated":"2017-09-14T06:24:54.000Z","comments":true,"path":"2017/09/13/Exploring big volume sensor data with Vroom/","link":"","permalink":"http://jinxuan.github.io/2017/09/13/Exploring big volume sensor data with Vroom/","excerpt":"Exploring big volume sensor data with VroomComments:ABSTRACT State of the art sensors within a single autonomous vehicle (AV) can produce video and LIDAR data at rates greater than 30 GB/hour.   Unsurprisingly, even small AV research teams can accumulate tens of terabytes of sensor data from multiple trips and multiple vehicles. AV practitioners would like to extract information about specic locations or specic situations for further study, but are often unable to. Queries over AV sensor data are dierent from generic analytics or spatial queries because they demand reasoning about elds of   view   as   well   as   heavy   computation   to   extract   features from scenes.   In this article and demo we present Vroom, a system for ad-hoc queries over AV sensor databases.  Vroom combines   domain   specic   properties   of   AV   datasets   with selective indexing and multi-query optimization to address challenges posed by AV sensor data.","text":"Exploring big volume sensor data with VroomComments:ABSTRACT State of the art sensors within a single autonomous vehicle (AV) can produce video and LIDAR data at rates greater than 30 GB/hour. Unsurprisingly, even small AV research teams can accumulate tens of terabytes of sensor data from multiple trips and multiple vehicles. AV practitioners would like to extract information about specic locations or specic situations for further study, but are often unable to. Queries over AV sensor data are dierent from generic analytics or spatial queries because they demand reasoning about elds of view as well as heavy computation to extract features from scenes. In this article and demo we present Vroom, a system for ad-hoc queries over AV sensor databases. Vroom combines domain specic properties of AV datasets with selective indexing and multi-query optimization to address challenges posed by AV sensor data. ##Introduction AV generate from high-resolution cameras, lidar and GPS at about 10 MBps. Queries: Q1 Compute basic statistics on recent trips such as data rates by sensor and location coverage. Q2 [building 3D maps] Retrieve all forward-facing video frames of the corner of Vassar and Main St. in Cambridge, MA., ordered clockwise. Q3 [ preparing labeled training] Retrieve lidar and video readings for all cameras in the vehicle, for intervals when any vehicle camera frame shows a bicycle. Group the data by trip, and order it by timestamp within each trip. Q4 [ preparing labeled training] Retrieve all sensor readings in the minute leading up to an interesting event, such as a possible near miss. e.g., where a vehicle’s CAN bus records a sudden brake or sharp steer, group the readings by trip and order them by timestamp within each trip. Challenges Computational intensity of UDFs: such as deep learning based classification Big volumes: ad-hoc query on large historical data Many features of interest: Interface and storage issues: Architecture Sophisticated feature precomputation and indexing: Synthesizing cheap predicates: Memoizing: Storage clustering, based on the workload Multi-query optimization: [to read] polystore data model System","raw":null,"content":null,"categories":[{"name":"paper reading","slug":"paper-reading","permalink":"http://jinxuan.github.io/categories/paper-reading/"}],"tags":[]},{"title":"pixiedust","slug":"pixiedust","date":"2017-06-07T22:22:55.000Z","updated":"2017-09-14T05:39:23.000Z","comments":true,"path":"2017/06/07/pixiedust/","link":"","permalink":"http://jinxuan.github.io/2017/06/07/pixiedust/","excerpt":"","text":"Data available at SF Open data. Tutorial slides: slidesshare Notebook: github","raw":null,"content":null,"categories":[],"tags":[]},{"title":"Spark Summit 2017","slug":"Spark summit 2017","date":"2017-06-06T07:00:00.000Z","updated":"2017-09-14T06:24:56.000Z","comments":true,"path":"2017/06/06/Spark summit 2017/","link":"","permalink":"http://jinxuan.github.io/2017/06/06/Spark summit 2017/","excerpt":"Spark summit 2017感觉这次比 年初的east 居然要寒碜?  居然早上连个早餐的都没有.   给的袋子里面也全部是广告. ","text":"Spark summit 2017感觉这次比 年初的east 居然要寒碜? 居然早上连个早餐的都没有. 给的袋子里面也全部是广告. KeyNoteA Deep Dive into Spark SQL’s Catalyst OptimizerHave already seen this talk during spark summit east. But still interesting to know we can define query plan ourselves. Todo: add code from slides later. Lazy Join Optimizations Without Upfront StatisticsProject from UCLA, This talk is really interesting. It is mainly trying to solve two problems. 1. Unlike traditional RDMBS, Generate statics for query optimization is hard. 2. Correct where clause sequence in join so that we could reduce the shuffle. The Top Five Mistakes Made When Writing Streaming ApplicationsAn useful talk. Talk about many good practice about writing spark streaming. What is the good and bad situtation for running spark streaming? Need wait summarize until slides published Building Data Product Based on Apache Spark at Airbnb Unify batch processing and streaming processing. Seems that this is really popular these days. Shared storge Question: Is it really a good idea to use spark streaming to calculate long window calculation? Experiences Migrating Hive Workload to SparkSQLFacebook is trying to migrate from Hive to Spark SQL to gain better performance. So that the user can wrote either HiveSQL or SparkSQL. This underlying engine will run and translate, estimate, or compare HiveQL to Spark SQL. I think this is too specific to facebook. They wouldn’t release how they do it and what performance gain they got. But I’m still couldn’ imagine how big the data running in those pipeline at facebook. Debugging Big Data Analytics in Apache Spark with BigDebugAnother research project from UCLA. This enhance apache spark with debug features such as set break point, watch point. And able to write function to change error input to midiate crash. They try to finish this to Spark 2.1 and currently they don’t spark SQL support. I kind like this idea. But it is hard to use. Assume we’re running a complex pipeline. This stop and check strategy is kind Productive Use of the Apache Spark PromptAnother talk about debug spark. The speaker from Mozilla shared some his thinking on debugging pattern. Taking Jupyter Notebooks and Apache Spark to the Next Level PixieDustAnother spark from Jupyter Notebook.","raw":null,"content":null,"categories":[{"name":"conference experience","slug":"conference-experience","permalink":"http://jinxuan.github.io/categories/conference-experience/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://jinxuan.github.io/tags/spark/"}]},{"title":"SRM 715","slug":"SRM-715","date":"2017-06-01T23:50:03.000Z","updated":"2017-09-14T06:11:40.000Z","comments":true,"path":"2017/06/01/SRM-715/","link":"","permalink":"http://jinxuan.github.io/2017/06/01/SRM-715/","excerpt":"Topcoder SRM 715Q1 : ImageCompression ","text":"Topcoder SRM 715Q1 : ImageCompression 123456789101112131415161718public class ImageCompression &#123; public String isPossible(String[] input, int k) &#123; boolean status = false; for(int i = 0; i &lt; input.length; i+=k) &#123; for(int j = 0; j &lt; input[0].length(); j+=k) &#123; char start = input[i].charAt(j); for(int l = i; l &lt; i + k; l++) for(int m = j; m &lt; j + k; m++) if (input[l].charAt(m) != start) &#123; System.out.println(l); System.out.println(m); return \"Impossible\"; &#125; &#125; &#125; return \"Possible\"; &#125;&#125; Q2: MaximumRangeDiv2 This question should be solved by greedy algorithm. But I didn’t realize that and choose to use brute force. I failed at only one test cases Q3) Failed to came up with the solution, I reference an accept user Abni_Saini: Code Ref: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#include &lt;bits/stdc++.h&gt;using namespace std;typedef long long int lld;#define rep(i,a,n) for(long long int i = (a); i &lt;= (n); ++i)#define repI(i,a,n) for(int i = (a); i &lt;= (n); ++i)#define repD(i,a,n) for(long long int i = (a); i &gt;= (n); --i)#define repDI(i,a,n) for(int i = (a); i &gt;= (n); --i)#define pb push_back#define mp make_pair#define ff first#define ss secondtypedef pair&lt;lld,lld&gt; PA;class InPrePost&#123;public: bool check(vector&lt;int&gt; A,vector&lt;int&gt; B)&#123; sort(A.begin(),A.end()); sort(B.begin(),B.end()); int n = A.size(); if(n!=B.size()) return false; rep(i,0,n-1) if(A[i]!=B[i]) return false; return true; &#125; bool tellMe(vector&lt;string&gt; S, vector&lt;int&gt; a1, vector&lt;int&gt; a2, vector&lt;int&gt; a3)&#123; // Check if there are elements not equal. if(!check(a1,a2) || !check(a1,a3)) return false; int n = a1.size(); if(n&lt;=1) return true; int root = a1[0]; vector&lt;int&gt; a1l,a1r,a2l,a2r,a3l,a3r; int leftn = -1; rep(i,0,n-1)&#123; if(a2[i]==root)&#123; leftn = i; &#125; else if(leftn==-1)&#123; a2l.pb(a2[i]); &#125; else a2r.pb(a2[i]); &#125; rep(i,1,n-1)&#123; if(i&lt;=leftn) a1l.pb(a1[i]); else a1r.pb(a1[i]); &#125; rep(i,0,n-2)&#123; if(i&lt;leftn) a3l.pb(a3[i]); else a3r.pb(a3[i]); &#125; map&lt;string,vector&lt;int&gt; &gt; M; M[S[0]] = a1l; M[S[2]] = a2l; M[S[4]] = a3l; if(!tellMe(S,M[\"pre\"],M[\"in\"],M[\"post\"])) return false; M[S[1]] = a1r; M[S[3]] = a2r; M[S[5]] = a3r; if(!tellMe(S,M[\"pre\"],M[\"in\"],M[\"post\"])) return false; return true; &#125; string isPossible(vector&lt;string&gt; S, vector&lt;int&gt; a1, vector&lt;int&gt; a2, vector&lt;int&gt; a3)&#123; if(tellMe(S,a1,a2,a3)) return \"Possible\"; else return \"Impossible\"; &#125;&#125;;","raw":null,"content":null,"categories":[],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://jinxuan.github.io/tags/algorithm/"}]}]}